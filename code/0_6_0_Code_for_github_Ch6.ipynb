{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u47lA5lDGTda",
        "outputId": "b35aba10-5696-42bd-81e0-8172817ea9f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.15.1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandana.loaders import osm\n",
        "import pandana, time, os, pandas as pd, numpy as np\n",
        "from pandas import read_excel, merge\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from shapely.geometry import Point\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"C:/Users/a/Anaconda/envs/ox/lib/site-packages/\")\n",
        "import pandana as pdna\n",
        "from pandana.loaders import osm\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"C:/Users/a/Anaconda/Lib/site-packages/\")\n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "import momepy\n",
        "\n",
        "import osmnx as ox\n",
        "ox.config(log_file=True, log_console=True, use_cache=True)\n",
        "ox.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke-EAZNuGTdg"
      },
      "source": [
        "# 1. Set up configuration of script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5WC0yQIGTdi"
      },
      "outputs": [],
      "source": [
        "distance = 750\n",
        "tags = '[\"highway\"~\"primary|secondary|tertiary|unclassified|residential|pedestrian|steps|path\"]'#|service\"]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPdW_rZ8GTdi"
      },
      "outputs": [],
      "source": [
        "ub_name= [\"Beimoor\",\"Ahrensburg Ost\",\"Ahrensburg West\",\"Alsterdorf\",\"Alter Teichweg\"\n",
        "          ,\"Berne\",\"Buchenkamp\",\"Buckhorn\",\"Farmsen\",\"Fuhlsbüttel\"\n",
        "          ,\"Fuhlsbüttel Nord\",\"Garstedt\",\"Großhansdorf\",\"Hallerstraße\",\"Hoisbüttel\",\n",
        "          \"Hudtwalckerstraße\",\"Kiekut\",\"Kiwittsmoor\", \"Klein Borstel\",\"Klosterstern\",\n",
        "          \"Langenhorn Nord\",\"Langenhorn Markt\",\"Lattenkamp\",\"Lohmühlenstraße\",\"Meiendorfer Weg\",\n",
        "          \"Meßberg\",\"Norderstedt Mitte\",\"Ochsenzoll\",\"Ohlsdorf\",\"Ohlstedt\",\n",
        "          \"Oldenfelde\",\"Richtweg\",\"Ritterstraße\",\"Schmalenbeck\", \"Sengelmannstraße\",\n",
        "          \"Steinstraße\", \"Stephansplatz\",\"Straßburger Straße\",\"Trabrennbahn\",\"Volksdorf\",\n",
        "          \"Wandsbek Markt\", \"Wandsbeker Chaussee\",\"Wartenau\",\"Jungfernstieg\",\"Hauptbahnhof Süd\",\n",
        "          \"Kellinghusenstraße\",\"Lübecker Straße\",\"Wandsbek-Gartenstadt\", \"Christuskirche\",\"Emilienstraße\",\n",
        "          \"Gänsemarkt\",\"Hagenbecks Tierpark\",\"Hagendeel\",\"Hellkamp\",\"Joachim-Mähl-Straße\",\n",
        "          \"Lutterothstraße\", \"Merkenstraße\",\"Messehallen\",\"Mümmelmannsberg\",\"Niendorf Markt\",\n",
        "          \"Niendorf Nord\",\"Osterstraße\",\"Schippelsweg\",\"Steinfurther Allee\", \"Schlump\",\n",
        "          \"Berliner Tor\",\"Billstedt\",\"Burgstraße\",\"Hammer Kirche\",\"Hauptbahnhof Nord\",\n",
        "          \"Horner Rennbahn\",\"Legienstraße\", \"Rauhes Haus\",\"Barmbek\", \"Baumwall\",\n",
        "          \"Borgweg\",\"Dehnhaide\",\"Eppendorfer Baum\",\"Feldstraße\",\"Habichtstraße\",\"Hamburger Straße\",\n",
        "          \"Hoheluftbrücke\",\"Landungsbrücken\",\"Mönckebergstraße\",\"Mundsburg\",\"Rathaus\",\n",
        "          \"Rödingsmarkt\",\"Saarlandstraße\",\"Sierichstraße\", \"St. Pauli\",\"Sternschanze\",\n",
        "          \"Uhlandstraße\",\"Elbbrücken\",\"HafenCity Universität\",\"Überseequartier\",\"Brückenstraße\",\n",
        "          \"Rothenburgsort\",\"Spaldingstraße\",\"Süderstraße\"]\n",
        "ub_point=[(53.674386,10.293294),(53.661389,10.243333),(53.664444,10.22),(53.606389,10.011667),(53.586667,10.064444)\n",
        "        ,(53.586667,10.064444),(53.652778,10.185833),(53.664722,10.155556),(53.607222,10.117778),(53.6325,10.026389)\n",
        "        ,(53.639722,10.016667),(53.684444,9.986111),(53.6625,10.286667),(53.573056,9.988889),(53.678056,10.1475)\n",
        "        ,(53.594444,9.996111),(53.653056,10.280278),(53.675,10.016111),(53.627778,10.033333),(53.581389,9.988333)\n",
        "        ,(53.660833,10.0175),(53.648889,10.017222),(53.599722,9.994722),(53.556389,10.018889),(53.638889,10.156111)\n",
        "        ,(53.5475,10.001389),(53.706944,9.9925),(53.678056,10.001389),(53.620556,10.031389),(53.694722,10.137222)\n",
        "        ,(53.618113,10.129416),(53.695833,9.989722),(53.567222,10.044722),(53.653056,10.260833),(53.609444,10.022222)\n",
        "        ,(53.55,10.005833),(53.558611,9.989167),(53.581944,10.068056),(53.598056,10.101944),(53.650556,10.163056)\n",
        "        ,(53.571944,10.067778),(53.57,10.058611), (53.564167,10.034444),(53.552222,9.993611),(53.552222,10.008333)\n",
        "        ,(53.588889,9.990833),(53.559722,10.028056),(53.592222,10.074167), (53.569444,9.962222),(53.571667,9.9525)\n",
        "        ,(53.555556,9.986944),(53.592778,9.943889),(53.603611,9.945),(53.579444,9.948889), (53.628889,9.949722)\n",
        "        ,(53.581944,9.9475),(53.538611,10.123889),(53.558333,9.976389),(53.528056,10.148056),(53.618889,9.951111)\n",
        "        ,(53.640556,9.950278),(53.576389,9.951667),(53.635278,9.9525),(53.541111,10.139167),(53.568056,9.97)\n",
        "        ,(53.553333,10.023889), (53.5425,10.107222),(53.555833,10.041944),(53.555556,10.055),(53.554167,10.006944)\n",
        "        ,(53.553889,10.085556),(53.5475,10.095278),(53.554167,10.066111),(53.587222,10.044444),(53.544167,9.981667)\n",
        "        ,(53.590833,10.015),(53.579167,10.040833),(53.583611,9.985), (53.556944,9.967778),(53.593611,10.051944)\n",
        "        ,(53.574444,10.037222),(53.577778,9.976389),(53.546111,9.970833),(53.551111,10.001389), (53.569444,10.0275)\n",
        "        ,(53.550278,9.993611),(53.548333,9.986944),(53.588889,10.0325),(53.590278,10.001111),(53.550556,9.97)\n",
        "        ,(53.563889,9.969167),(53.564722,10.026667),(53.534722,10.023333),(53.540278,10.008611),(53.540556,9.998889)\n",
        "        ,(53.541111,10.035),(53.538889,10.043611),(53.549167,10.0175),(53.543889,10.022222)]\n",
        "sb_name=[\"Hamburg Airport (Flughafen)\",\"Iserbrook\",\"Rissen\",\"Sülldorf\",\"Wedel\",\n",
        "         \"Jungfernstieg\",\"Königstraße\",\"Landungsbrücken\",\"Reeperbahn\",\"Stadthausbrücke\",\n",
        "         \"Alte Wöhr\",\"Bahrenfeld\",\"Barmbek\",\"Blankenese\",\"Friedrichsberg\",\n",
        "\n",
        "         \"Hasselbrook\",\"Hochkamp\",\"Hoheneichen\",\"Klein Flottbek\",\"Kornweg\",\n",
        "         \"Landwehr\", \"Ohlsdorf\", \"Othmarschen\",\"Poppenbüttel\",\"Rübenkamp\",\n",
        "\n",
        "         \"Wandsbeker Chaussee\",\"Wandsbeker Chaussee\",\"Wellingsbüttel\",\"Altona\",\"Hauptbahnhof Süd\",\n",
        "\n",
        "         \"Hauptbahnhof Nord\",\"Berliner Tor\", \"Allermöhe\",\"Bergedorf\",\"Billwerder-Moorfleet\",\n",
        "         \"Mittlerer Landweg\",\"Nettelnburg\",\"Rothenburgsort\",\"Tiefstack\",\"Agathenburg\",\n",
        "         \"Buxtehude\", \"Dollern\",\"Fischbek\",\"Halstenbek\",\"Horneburg\",\n",
        "         \"Krupunder\", \"Neu Wulmstorf\",\"Neukloster\",\"Pinneberg\",\"Stade\",\n",
        "         \"Thesdorf\", \"Hammerbrook\", \"Harburg\",\"Harburg Rathaus\",\"Heimfeld\",\n",
        "         \"Neugraben\", \"Neuwiedenthal\",\"Veddel\",\"Wilhelmsburg\",\"Elbbrücken\",\n",
        "         \"Dammtor\", \"Holstenstraße\",\"Sternschanze\",\"Aumühle\",\"Reinbek\",\n",
        "         \"Wohltorf\", \"Diebsteich\", \"Elbgaustraße\",\"Eidelstedt\",\"Langenfelde\",\"Stellingen\"\n",
        "        ]\n",
        "sb_point=[(53.632222,10.006389),(53.576389,9.814722),(53.583056,9.7525),(53.581111,9.797222),(53.581944,9.705),\n",
        "          (53.552222,9.993611),(53.547222,9.942778),(53.546111,9.970833),(53.549444,9.958056),(53.549722,9.986111),\n",
        "          (53.5975,10.035556),(53.56,9.910556),(53.5875,10.043889),(53.564444,9.815),(53.576389,10.0575),\n",
        "\n",
        "          (53.564722,10.055833),(53.561389,9.841944),(53.635556,10.068056),(53.558056,9.860556),(53.6325,10.054444),\n",
        "          (53.561111,10.037778),(53.620556,10.031667),(53.559167,9.886111),(53.651944,10.093889),(53.606389,10.033056),\n",
        "\n",
        "          (53.57016,10.05973),(53.569722,10.06),(53.641111,10.0825),(53.551944,9.934444),(53.552778,10.006389),\n",
        "\n",
        "          (53.553056,10.0075),(53.552778,10.024167),(53.490278,10.158611),(53.49,10.206389),(53.515833,10.096389),\n",
        "          (53.497778,10.131667),(53.487778,10.181111),(53.538611,10.043333),(53.531111,10.065833),(53.564722,9.53),\n",
        "          (53.470556,9.688056),(53.545833,9.557778),(53.474722,9.820278),(53.630556,9.838889),(53.508889,9.576389),\n",
        "          (53.615833,9.867778),(53.473056,9.788333),(53.481389,9.639444),(53.654722,9.798333),(53.596111,9.4775),\n",
        "          (53.644444,9.813611),(53.546389,10.023889),(53.456389,9.991667),(53.460556,9.981389),(53.465556,9.962778),\n",
        "          (53.474167,9.853056),(53.473056,9.877222),(53.521944,10.013611),(53.498056,10.006667),(53.534692,10.024689),\n",
        "          (53.560833,9.989722),(53.561944,9.949167),(53.563611,9.965833),(53.53,10.314722),(53.50828,10.2529),\n",
        "          (53.520556,10.277778),(53.568056,9.934444),(53.602778,9.893056),(53.596111,9.906389),(53.579722,9.930833),\n",
        "          (53.589722,9.918333)\n",
        "         ]\n",
        "hh_u_station_name= ub_name + sb_name\n",
        "hh_u_station_point = ub_point + sb_point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPfaYvNwGTdj"
      },
      "source": [
        "# 2.Calculate the indicators & stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oju5Ub4_GTdk"
      },
      "outputs": [],
      "source": [
        "#=====Create a Dataframe with column names:\n",
        "df_all = pd.DataFrame()\n",
        "#=====Create list of street network\n",
        "num_node_list=[]\n",
        "street_length_list=[]\n",
        "k_list=[]\n",
        "edge_length_total_list=[]\n",
        "edge_length_avg_list=[]\n",
        "clustering_coefficient_list=[]\n",
        "cc_avg_list=[]\n",
        "bc_avg_list=[]\n",
        "\n",
        "#=====Create list of POIs\n",
        "mean_pois_per_node=[]\n",
        "# num_pois_near_station=[]\n",
        "\n",
        "for i in range(0, len(hh_u_station_point)):\n",
        "    #-----------Netowrk analysis\n",
        "    G = ox.graph_from_point(hh_u_station_point[i],\n",
        "                            dist=distance, network_type='none',\n",
        "                            custom_filter = tags)\n",
        "    #-----------Point geometry of sthe stations\n",
        "    df_all.loc[hh_u_station_name[i], \"x\"] = hh_u_station_point[i][0]\n",
        "    df_all.loc[hh_u_station_name[i], \"y\"] = hh_u_station_point[i][1]\n",
        "    df_all.loc[hh_u_station_name[i], \"geometry\"] = str(Point(hh_u_station_point[i][1], hh_u_station_point[i][0]))\n",
        "\n",
        "    #=====Calculate stats\n",
        "    basic_stats = ox.basic_stats(G,  area=None, clean_intersects=True, tolerance=15, circuity_dist='gc')\n",
        "    extended_stats = ox.extended_stats(G, connectivity=True, anc=False, ecc=False, bc=True, cc=True)\n",
        "\n",
        "    #=====Calculate Street orientation=======================\n",
        "    G = ox.add_edge_bearings(G)\n",
        "    bearings = pd.Series([data['bearing'] for u, v, k, data in G.edges(keys=True, data=True)])\n",
        "\n",
        "    #Calculate entropy\n",
        "    from scipy.stats import entropy\n",
        "    pd_series = pd.Series(bearings)\n",
        "    counts = pd_series.value_counts()\n",
        "    entropy_count = entropy(counts)\n",
        "\n",
        "        #####DataFrame: 垂直增加cases\n",
        "    hh_u_station_name_pd = pd.Series(hh_u_station_name[i])     #=====Covert list of \"station names\" into dataframe\n",
        "    df_all.loc[hh_u_station_name[i], \"number of node\"] = np.array(basic_stats['n'])\n",
        "    df_all.loc[hh_u_station_name[i], \"number of link\"] = np.array(basic_stats['m'])\n",
        "    df_all.loc[hh_u_station_name[i], \"node_degree_avg\"] = np.array(basic_stats['k_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], 'sum of all edge lengths in the graph(m)'] = np.array(basic_stats['edge_length_total'])\n",
        "    df_all.loc[hh_u_station_name[i], \"edge_length_avg\"] = np.array(basic_stats['edge_length_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], 'street_length_total'] = np.array(basic_stats['street_length_total'])\n",
        "    df_all.loc[hh_u_station_name[i], \"street_length_avg\"] = np.array(basic_stats['street_length_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], 'self_loop_proportion'] = np.array(basic_stats['self_loop_proportion'])\n",
        "    df_all.loc[hh_u_station_name[i], 'clean_intersection_count '] = np.array(basic_stats['clean_intersection_count'])\n",
        "    df_all.loc[hh_u_station_name[i], 'clean_intersection_density_km'] = np.array(basic_stats['clean_intersection_density_km'])\n",
        "    df_all.loc[hh_u_station_name[i], 'number of intersections'] = np.array(basic_stats['intersection_count'])\n",
        "    df_all.loc[hh_u_station_name[i], 'link-node ratio'] = np.array(basic_stats['streets_per_node_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], 'street_segments_count'] = np.array(basic_stats['street_segments_count'])\n",
        "    df_all.loc[hh_u_station_name[i], 'node_density_km'] = np.array(basic_stats['node_density_km'])\n",
        "    df_all.loc[hh_u_station_name[i], 'intersection_density_km'] = np.array(basic_stats['intersection_density_km'])\n",
        "    df_all.loc[hh_u_station_name[i], 'edge_density_km'] = np.array(basic_stats['edge_density_km'])\n",
        "    df_all.loc[hh_u_station_name[i], 'street_density_km'] = np.array(basic_stats['street_density_km'])\n",
        "    df_all.loc[hh_u_station_name[i], 'circuity_avg'] = np.array(basic_stats['circuity_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], \"entropy of street bearing\"] = np.array(entropy_count)\n",
        "    df_all.loc[hh_u_station_name[i], \"clustering_coefficient_avg\"] = np.array(extended_stats['clustering_coefficient_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], \"cc_avg\"] = np.array(extended_stats['closeness_centrality_avg'])\n",
        "    df_all.loc[hh_u_station_name[i], \"bc_avg\"] = np.array(extended_stats['betweenness_centrality_avg'])\n",
        "\n",
        "    #------Dataframe 2.df_cluster_sorted, 3.df_cc_sorted, 4.df_bc_sorted, 5.df_bearings_sorted----------\n",
        "    #=====Covert data into list: Node level\n",
        "    cluster=extended_stats['clustering_coefficient']\n",
        "    cc=extended_stats['closeness_centrality']\n",
        "    bc=extended_stats['betweenness_centrality']\n",
        "\n",
        "    #=====Covert data into dataframe\n",
        "    df_cluster_sorted = pd.DataFrame(data=pd.Series(cc).sort_values(), columns=['cluster'])\n",
        "    df_cc_sorted = pd.DataFrame(data=pd.Series(cc).sort_values(), columns=['cc'])\n",
        "    df_bc_sorted = pd.DataFrame(data=pd.Series(bc).sort_values(), columns=['bc'])\n",
        "\n",
        "    df_bearings_sorted = pd.DataFrame(data=bearings.sort_values(), columns=['bearings'])\n",
        "\n",
        "    #--------POIs---------------------------------------\n",
        "    #===========1. bounding box of the network==========================\n",
        "    north, south, east, west = ox.utils_geo.bbox_from_point(hh_u_station_point[i], dist = distance, project_utm=False) #If lat & lon are desired : project_utm=Fale\n",
        "    north, south, east, west\n",
        "\n",
        "    ##### bounding box as a list of llcrnrlat, llcrnrlng, urcrnrlat, urcrnrlng\n",
        "    bbox = [south, west , north, east] #lat-long bounding box for berkeley/oakland\n",
        "\n",
        "    ##### configure filenames to save/load POI and network datasets\n",
        "    bbox_string = '_'.join([str(x) for x in bbox])\n",
        "    net_filename = 'data/network_{}.h5'.format(bbox_string)\n",
        "\n",
        "    #####==========2. POI================\n",
        "    ##### configure search at a max distance of 1 km for up to the 10 nearest points-of-interest\n",
        "    # amenities = ['bar','cafe', 'pub','restaurant', 'supermarket']\n",
        "\n",
        "    num_pois = 10\n",
        "    num_categories = len(amenities) + 1 #one for each amenity, plus one extra for all of them combined\n",
        "\n",
        "    # poi_filename = 'data/pois_{}_{}.csv'.format('_'.join(amenities), bbox_string)\n",
        "    poi_filename = 'data/pois'+'_'+hh_u_station_name[i]+'.csv'\n",
        "\n",
        "    #=====Retrieve POIs data\n",
        "    # start_time = time.time()\n",
        "#     if os.path.isfile(poi_filename):\n",
        "#         ##### if a points-of-interest file already exists, just load the dataset from that\n",
        "#         pois = pd.read_csv(poi_filename)\n",
        "#         method = 'loaded from CSV'\n",
        "#     else:\n",
        "        ##### otherwise, query the OSM API for the specified amenities within the bounding box\n",
        "    osm_tags = '\"amenity\"~\"{}\"'.format('|'.join(amenities))\n",
        "\n",
        "    pois = osm.node_query(bbox[0], bbox[1], bbox[2], bbox[3], tags=osm_tags)\n",
        "\n",
        "    if len(pois) == 0 :\n",
        "        disp('All values are below the limit.')\n",
        "    else:\n",
        "            ##### using the '\"amenity\"~\"school\"' returns preschools etc, so drop any that aren't just 'school' then save to CSV\n",
        "        pois = pois[pois['amenity'].isin(amenities)]\n",
        "        pois.to_csv(poi_filename, index=False, encoding='utf-8')\n",
        "        method = 'downloaded from OSM'\n",
        "\n",
        "        # print('{:,} POIs {} in {:,.2f} seconds'.format(len(pois), method, time.time()-start_time))\n",
        "        # pois[['amenity', 'name', 'lat', 'lon']].head()\n",
        "\n",
        "    #####==========3. Retrieve network data================\n",
        "    network = osm.pdna_network_from_bbox(bbox[0], bbox[1], bbox[2], bbox[3])\n",
        "\n",
        "    ##### identify nodes that are connected to fewer than some threshold of other nodes within a given distance\n",
        "    ##### do nothing with this for now, but see full example in other notebook for more\n",
        "    lcn = network.low_connectivity_nodes(impedance=1000, count=10, imp_name='distance')\n",
        "\n",
        "    ##### precomputes the range queries (the reachable nodes within this maximum distance)\n",
        "    ##### so, as long as you use a smaller distance, cached results will be used\n",
        "    network.precompute(distance + 1)\n",
        "\n",
        "    ##### initialize the underlying C++ points-of-interest engine\n",
        "    network.init_pois(num_categories=num_categories, max_dist=distance, max_pois=num_pois)\n",
        "\n",
        "    ##### initialize a category for all amenities with the locations specified by the lon and lat columns\n",
        "    network.set_pois(category='all', x_col=pois['lon'], y_col=pois['lat'])\n",
        "\n",
        "    # =======3. For each node, measure the distance between this node and the n number of POIs=================\n",
        "\n",
        "    ###### searches for the n nearest amenities (of all types) to each node in the network (Here n=num_pois=10)\n",
        "    all_access = network.nearest_pois(distance=distance, category='all', num_pois=num_pois)\n",
        "\n",
        "    ##### it returned a df with the number of columns equal to the \"NUMBER OF POIs\" that are requested\n",
        "    ##### each cell represents the NETWORK DISTANCE from the node to each of the n POIs\n",
        "    print('{:,} nodes'.format(len(all_access)))\n",
        "    all_access.head()\n",
        "\n",
        "    # =======Use the coordicate of the POIs to get the node id\n",
        "    ##### Get node id\n",
        "    pois_nodes = network.get_node_ids(pois.lon, pois.lat)\n",
        "    #####save the node id in a column in the data of \"network\"\n",
        "    network.set(pois_nodes, name = 'pois_nodes')\n",
        "\n",
        "    ######=====File 1: One column for one station ===========================================\n",
        "    accessibility = network.aggregate(distance = 750,\n",
        "                                          type = 'count',\n",
        "                                          name = 'pois_nodes')\n",
        "    df_accessibility  = pd.DataFrame.from_dict(accessibility)\n",
        "#     df_accessibility.to_excel(writer_accessibility, sheet_name='Sheet1', startcol=i*2, startrow=1, header = False, index = True)\n",
        "    hh_u_station_name_pd = pd.Series(hh_u_station_name[i])\n",
        "#     hh_u_station_name_pd.to_excel(writer_accessibility, sheet_name='Sheet1', startcol=i*2, startrow=0, header = False, index = False)\n",
        "\n",
        "    #######=====File 2: one column for all stations===========================================\n",
        "    ######At network level: how many POIs on average are within 750m of the nodes?\n",
        "    accessibility_describe  = accessibility.describe()\n",
        "    df_accessibility_describe = pd.DataFrame.from_dict(accessibility_describe)\n",
        "    mean_pois = df_accessibility_describe.loc['mean']\n",
        "    mean_pois_per_node.append(mean_pois)\n",
        "    df_mean_pois  = pd.DataFrame.from_dict(mean_pois)\n",
        "\n",
        "    #====Destination accessibility\n",
        "    df_accessibility_describe.loc['mean']\n",
        "\n",
        "    #=====Density of POIs\n",
        "    len(pois)\n",
        "\n",
        "    #=====Diversity of POIs\n",
        "    len(pois.groupby(['amenity']).size())\n",
        "\n",
        "    #=====Include POIs in to dataframe \"df_all\"\n",
        "    df_all.loc[hh_u_station_name[i], \"POIs_Accessibility\"] = np.array(mean_pois)\n",
        "    df_all.loc[hh_u_station_name[i], \"POIs_Density\"] = np.array(len(pois))\n",
        "    df_all.loc[hh_u_station_name[i], \"POIs_Diversity\"] = np.array(len(pois.groupby(['amenity']).size()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHqHTZfGTdl"
      },
      "source": [
        "# 3. Select subset for clustering analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xDrMEUwGTdl"
      },
      "outputs": [],
      "source": [
        "#=====Select columns from combined df to prepare for clustering analysis\n",
        "df_all_selected = df_all[['number of node' , 'number of link'\n",
        "                         , 'node_degree_avg', 'street_length_avg'\n",
        "                         , 'entropy of street bearing', 'clustering_coefficient_avg'\n",
        "                         , 'cc_avg', 'bc_avg'\n",
        "                         , 'POIs_Accessibility', 'POIs_Density'\n",
        "                         , 'POIs_Diversity'\n",
        "                         ]]\n",
        "#=====Set osmid as index: https://stackoverflow.com/questions/42196337/dataframe-set-index-not-setting\n",
        "# df_all_selected.set_index('osmid', inplace=True, drop=True)\n",
        "len(df_all_selected), len(df_all_selected), df_all_selected\n",
        "\n",
        "# Covert dataframe into array\n",
        "df_all_selected_array = df_all_selected.to_numpy()\n",
        "df_all_selected_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzlqCxbUGTdm"
      },
      "source": [
        "# 4. Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVRWA2IGTdm"
      },
      "outputs": [],
      "source": [
        "# ===== Normalize data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = df_all_selected\n",
        "X_std = StandardScaler().fit_transform(X) # normalizing the data\n",
        "names  = ['number of node' , 'number of link'\n",
        "                         , 'node_degree_avg', 'street_length_avg'\n",
        "                         , 'entropy of street bearing', 'clustering_coefficient_avg'\n",
        "                         , 'cc_avg', 'bc_avg'\n",
        "                         , 'POIs_Accessibility', 'POIs_Density'\n",
        "                         , 'POIs_Diversity']\n",
        "X_std_df = pd.DataFrame(X_std, columns=names)\n",
        "X_std_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8pGJIY9GTdm"
      },
      "source": [
        "# 5. Check correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kzeMVVDGTdn"
      },
      "outputs": [],
      "source": [
        "# ===== Check correlations\n",
        "X = X_std_df\n",
        "X.describe()\n",
        "correlation = X.corr()\n",
        "\n",
        "# ===== Plot the correlation\n",
        "correlation.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaRA76vGTdn"
      },
      "source": [
        "# 6. Affinity Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zC4tj71GTdn"
      },
      "outputs": [],
      "source": [
        "# https://blog.csdn.net/manjhOK/article/details/79586791\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# #############################################################################\n",
        "# Define dataSet\n",
        "X= df_all_selected_array\n",
        "# X= PCA_components.iloc[:,:2].to_numpy() # 要先從dataFrame轉成array\n",
        "prf = -500000  #如果feature/indicator 太少要調整\n",
        "######Cluster 0: -500000; Cluster 1: -1000000; Cluster 2: -500000\n",
        "\n",
        "# # # Generate sample data\n",
        "# centers = [[1, 1], [-1, -1], [1, -1]]\n",
        "# X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.5, random_state=0\n",
        "#                            , n_features=4)\n",
        "# prf = -50\n",
        "# #############################################################################\n",
        "# Define parameters\n",
        "# #############################################################################\n",
        "# # Compute Affinity Propagation\n",
        "# af = AffinityPropagation(\n",
        "#                         ).fit(X)\n",
        "#=====2. define the model\n",
        "model = AffinityPropagation(preference= prf, # Decide number of clusters\n",
        "                         damping=0.5,\n",
        "                         max_iter=200, convergence_iter=15, copy=True, affinity='euclidean', verbose=False, random_state='warn'\n",
        "                         ) #The number of clusters is influenced by the preference values and the message-passing procedure.\n",
        "#=====3. fit / train the model\n",
        "af= model.fit(X)\n",
        "#=====4. Reurn an array of cluster labels / assign a cluster label to each example\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# #############################################################################\n",
        "# Check attributes\n",
        "#=====有幾種類標籤? retrieve unique clusters / Return an array of cluster labels\n",
        "clusters = unique(yhat)\n",
        "#=====所有data point 的类标签，Retrieve array of cluster labels.\n",
        "labels = af.labels_   #Returns ndarray of shape (n_samples,).  Labels of each point.\n",
        "# ===== 有幾種label，每個label有幾個data point\n",
        "unique_labels, counts_labels = np.unique(labels, return_counts=True)\n",
        "#=====frequency counts for unique values in an array\n",
        "np.array(np.unique(labels, return_counts=True)).T\n",
        "#=====类中心的位置，Who/ which datapoint is the exampler.\n",
        "cluster_centers_indices = af.cluster_centers_indices_ ##Returns ID of data point. Returns ndarray of shape (n_clusters,). Indices of cluster centers.\n",
        "#=====類中心的features。Show features of all the examplers.\n",
        "cluster_centers  = af.cluster_centers_    # Return the features/attributes of the datapoint. Return ndarray of shape (n_clusters, n_features). Cluster centers (if affinity != precomputed).\n",
        "#=====Show features of the first exampler\n",
        "# X[cluster_centers_indices[0], :]  # Show all the features of the first examplar\n",
        "# af.cluster_centers_[0, :] # Show all the features of the first examplar\n",
        "#=====Calculate number of features\n",
        "# num_features = len(af.cluster_centers_[0, :] ) #Number of features\n",
        "#=====\n",
        "af.affinity_matrix_ ##最后输出的A矩阵，ndarray of shape (n_samples, n_samples). Stores the affinity matrix used in fit.\n",
        "#=====\n",
        "af.n_iter_ ##迭代次数，int. Number of iterations taken to converge.\n",
        "\n",
        "# Plot result\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "plt.close('all')\n",
        "plt.figure(1)\n",
        "plt.clf()\n",
        "\n",
        "# colors = ['b','g','r', 'c', 'm', 'y', 'k', 'forestgreen','darkblue','orange', 'violet']\n",
        "colors =['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', 'black']\n",
        "# colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
        "\n",
        "for k, col in zip(range(n_clusters_), colors):\n",
        "    class_members = labels == k\n",
        "    cluster_center = X[cluster_centers_indices[k]]\n",
        "    plt.plot(X[class_members, 0], X[class_members, 1], col)\n",
        "#     plt.plot(X[class_members, 0], X[class_members, 1], col + '.') # if use colors cycle\n",
        "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
        "             markeredgecolor='k', markersize=14)\n",
        "    for x in X[class_members]:\n",
        "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
        "\n",
        "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "plt.xlabel('PCA score')\n",
        "plt.ylabel('PCA score')\n",
        "plt.show()\n",
        "n_clusters_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a66vBcKGTdn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akPNMJIbGTdn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
